{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddf2d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Set up the Selenium driver\n",
    "driver = webdriver.Edge(executable_path=r'path_to_your_webdriver')\n",
    "\n",
    "# Step 1: Log in\n",
    "login_url = 'your_login_url'\n",
    "driver.get(login_url)\n",
    "username_field = driver.find_element(By.NAME, 'username_field')\n",
    "password_field = driver.find_element(By.NAME, 'password_field')\n",
    "username_field.send_keys('your_username')\n",
    "password_field.send_keys('your_password')\n",
    "password_field.submit()  # submit the form\n",
    "time.sleep(5)  # wait for the page to load\n",
    "\n",
    "# Step 2: Go to the target page\n",
    "target_page_url = 'your_target_page_url'\n",
    "driver.get(target_page_url)\n",
    "time.sleep(5)  # wait for the page to load\n",
    "\n",
    "# Parse the page source with BeautifulSoup\n",
    "page_source = driver.page_source\n",
    "soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "# Find all 'td' elements that contain specific names (update regex with desired patterns)\n",
    "name_tds = soup.find_all(\n",
    "    'td',\n",
    "    text=re.compile(\n",
    "        r'Name1|Name2|Name3|...'  # Add all needed names here\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create a DataFrame to store all data\n",
    "df = pd.DataFrame(columns=['Column1', 'Column2', 'Column3', 'Column4'])  # Customize column names as needed\n",
    "\n",
    "# Iterate over each 'name_td'\n",
    "for name_td in name_tds:\n",
    "    # If we found such a tag, we get the next 'td' tag\n",
    "    if name_td:\n",
    "        next_td = name_td.find_next_sibling('td')\n",
    "\n",
    "        # If the next 'td' tag exists, find the 'a' tag within it and get its 'href'\n",
    "        if next_td:\n",
    "            a = next_td.find('a', href=True)\n",
    "            if a:\n",
    "                target_link = a['href']\n",
    "\n",
    "                # Construct the full URL\n",
    "                full_url = 'your_base_url' + target_link\n",
    "\n",
    "                # Navigate to the target link\n",
    "                driver.get(full_url)\n",
    "                time.sleep(5)\n",
    "\n",
    "                # Parse the page source with BeautifulSoup\n",
    "                page_source = driver.page_source\n",
    "                soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "                # Scrape the required data for each row\n",
    "                # Find tbody first\n",
    "                tbody = soup.find('tbody')\n",
    "\n",
    "                data = {'Column1': [], 'Column2': [], 'Column3': [], 'Column4': []}  # Adjust as needed\n",
    "\n",
    "                if tbody:\n",
    "                    rows = tbody.find_all('tr')\n",
    "                    for row in rows:\n",
    "                        # Example logic to filter and extract data\n",
    "                        first_td = row.find('td')\n",
    "                        if first_td and first_td.text.strip() == 'ExcludeCondition':\n",
    "                            continue\n",
    "                        if row.find('form') is None or row.find('input') is None:\n",
    "                            continue\n",
    "\n",
    "                        # Change the indices to match actual column positions\n",
    "                        column1_data = row.find_all('td')[0].text if row.find_all('td') else None\n",
    "                        column2_data = row.find('a', target='newtop')['href'] if row.find('a', target='newtop') else None\n",
    "                        column3_data = row.find_all('td')[2].text if len(row.find_all('td')) > 2 else None\n",
    "\n",
    "                        # Store the data\n",
    "                        data['Column1'].append(name_td.text)\n",
    "                        data['Column2'].append(column1_data)\n",
    "                        data['Column3'].append(column2_data)\n",
    "                        data['Column4'].append(column3_data)\n",
    "\n",
    "                    # Convert the data to a DataFrame\n",
    "                    df = df.append(pd.DataFrame(data), ignore_index=True)\n",
    "\n",
    "driver.quit()  # close the browser when done\n",
    "\n",
    "# Process the 'Column3' column if necessary (Example provided)\n",
    "# df[['Discard', 'Column3']] = df['Column3'].str.split('some_pattern=', expand=True)\n",
    "# df = df.drop(columns=['Discard'])\n",
    "\n",
    "# Example: Filter out already processed items\n",
    "try:\n",
    "    processed_ids_df = pd.read_excel('path_to_existing_file.xlsx')\n",
    "    processed_ids = processed_ids_df['identifier_column'].tolist()\n",
    "except FileNotFoundError:\n",
    "    processed_ids = []\n",
    "\n",
    "df = df[~df['Column3'].isin(processed_ids)]\n",
    "\n",
    "# Example: Count occurrences of specific names in 'Column4'\n",
    "names = ['Name1', 'Name2', 'Name3', ... ]\n",
    "counts = {name: df['Column4'].str.contains(name, case=False).sum() for name in names}\n",
    "counts_df = pd.DataFrame.from_dict(counts, orient='index', columns=['Count'])\n",
    "counts_df.index.name = 'Identifier'\n",
    "\n",
    "# Example: Common errors in 'Column4' using n-grams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 6), max_features=1000)\n",
    "word_vectors = vectorizer.fit_transform(df['Column4'])\n",
    "words = vectorizer.get_feature_names()\n",
    "counts = word_vectors.sum(axis=0).A1\n",
    "\n",
    "common_errors_df = pd.DataFrame({'Common Error': words, 'Counts': counts})\n",
    "for name in names:\n",
    "    common_errors_df = common_errors_df[~common_errors_df['Common Error'].str.contains(name, case=False)]\n",
    "common_errors_df = common_errors_df.nlargest(100, 'Counts')\n",
    "\n",
    "# Export the DataFrame and analysis to an Excel file\n",
    "output_path = 'output_file.xlsx'\n",
    "with pd.ExcelWriter(output_path) as writer:\n",
    "    df.to_excel(writer, sheet_name='Scraped Data', index=False)\n",
    "    counts_df.reset_index().to_excel(writer, sheet_name='Counts', index=False)\n",
    "    common_errors_df.to_excel(writer, sheet_name='Common Errors', index=False)\n",
    "\n",
    "# Example: Count occurrences of key phrases\n",
    "key_phrases = ['key_phrase_1', 'key_phrase_2', ...]\n",
    "key_phrase_counts = {phrase: df['Column4'].str.contains(phrase, case=False, regex=True).sum() for phrase in key_phrases}\n",
    "key_phrase_counts_df = pd.DataFrame.from_dict(key_phrase_counts, orient='index', columns=['Count'])\n",
    "key_phrase_counts_df.index.name = 'Key Phrase'\n",
    "\n",
    "# Save the updated processed IDs\n",
    "processed_ids.extend(df['Column3'].tolist())\n",
    "processed_ids_df = pd.DataFrame(processed_ids, columns=['identifier_column'])\n",
    "processed_ids_df.to_excel('path_to_existing_file.xlsx', index=False)\n",
    "\n",
    "# Save the key phrase counts DataFrame to the same Excel file\n",
    "with pd.ExcelWriter(output_path, engine='openpyxl', mode='a') as writer:\n",
    "    key_phrase_counts_df.reset_index().to_excel(writer, sheet_name='Key Phrase Counts', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
